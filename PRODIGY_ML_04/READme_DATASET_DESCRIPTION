This repository contains a hand gesture recognition model leveraging deep learning techniques to enable intuitive human-computer interaction and gesture-based control systems. The model is trained on a dataset of near-infrared images from the Leap Motion sensor, encompassing 10 different hand gestures performed by 10 subjects. Using the TensorFlow and Keras libraries, the project preprocesses the data with image augmentation and normalization techniques, and employs a Convolutional Neural Network (CNN) for classification. The architecture includes multiple convolutional and pooling layers followed by dense layers, achieving robust feature extraction and classification. The model, trained on a subset of 1000 images, attains high accuracy, enabling real-time prediction of hand gestures. A user-friendly GUI built with Tkinter allows easy image upload and real-time gesture recognition. The project demonstrates the efficacy of combining CNNs with infrared imagery for advanced gesture recognition applications.
